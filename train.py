import os

import torch
import typer
import numpy as np
from torch import Tensor, nn
from torch.optim import AdamW
from datetime import datetime

# from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from data import musdbDataset
from splitter import Splitter
from util import utility

DISABLE_TQDM = os.environ.get("DISABLE_TQDM", False)
app = typer.Typer(pretty_exceptions_show_locals=False)

def weight_reset(m):
    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
        m.reset_parameters()

def spectrogram_loss(masked_target: Tensor, original: Tensor) -> Tensor:
    """
    masked_target (Tensor): a masked STFT generated by applying a net's
        estimated mask for source S to the ground truth STFT for source S
    original (Tensor): an original input mixture
    """
    # square_difference = torch.square(masked_target - original)
    # loss_value = torch.mean(square_difference)

    util = utility()

    ### STFT ###
    # mse_loss = nn.functional.mse_loss()
    masked_mag = torch.abs(masked_target)
    masked_mag_db = util.mag2db(masked_mag)
    target_mag_db = util.mag2db(original)
    loss_real = nn.functional.mse_loss(masked_mag_db, target_mag_db)
    loss_value = loss_real

    # ### Mel Spec ###
    # target_mel, target_mel_db = util.mel_spec(y_target_wav, time_domain=True)
    # est_mel, est_mel_db = util.mel_spec(masked_stft, time_domain=False)
    # loss_mel = torch.nn.functional.mse_loss(est_mel, target_mel)
    # # mae_loss = nn.L1Loss() # MAE Loss
    # # loss = mae_loss(est_mel, target_mel)
    # loss_value = loss_mel
    return loss_value

def train(
    dataset: str = "/home/ytang363/7100_spr2023/musdb18",
    output_dir: str = "/home/ytang363/7100_spr2023/model",
    epochs: int = 300,
    batch_size: int = 16,
    batch_size_val: int = 8,
    learning_rate: float = 1e-4,
    drop: bool = True,
    pretrain: bool = False,
    model_dir: str = "/home/ytang363/7100_spr2023/model/20230409-02_ep-300_b-4.pt",
    stem_number: int = 4
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    util = utility()
    print("total epochs: {}; batch size: {}".format(epochs, batch_size))

    train_dataset = musdbDataset(root=dataset, is_train=True, split="train")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=drop)
    validation_dataset = musdbDataset(root=dataset, is_train=True, split="valid")
    validation_loader = DataLoader(validation_dataset, batch_size=batch_size_val, shuffle=True, drop_last=drop)

    model = Splitter(stem_names=[s for s in train_dataset.targets]).to(device)
    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=1e-5)
    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=100,threshold=1e-1,min_lr=1e-7)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.1)

    # Continue Training
    if pretrain:
        print("Continue Training")
        checkpoint = torch.load(model_dir)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler'])
        loss = checkpoint['loss']

    now_str = datetime.now().strftime("%Y%m%d-%H")
    writer = SummaryWriter("/home/ytang363/7100_spr2023/logs/{}".format(now_str))
    val_writer = SummaryWriter("/home/ytang363/7100_spr2023/logs/{}_val".format(now_str))
    
    for epoch in range(epochs):
        print("Epoch {}:".format(epoch+1))

        ## Training ##
        model.train(True)
        running_loss = 0
        last_loss = 0
        batch_iterator = tqdm(train_loader, desc="Batch")
        for batch_idx, batch in enumerate(batch_iterator):
            x_wav_batch, y_target_wavs_batch = batch
            optimizer.zero_grad()

            for i in range(batch_size):
                x_wav = x_wav_batch[i].to(device)
                predictions = model(x_wav)
                stem_losses = []

                for name, masked_stft in predictions.items():
                    y_target_wav = y_target_wavs_batch[name][i].to(device)

                    ### STFT ###
                    target_stft, target_mag, target_phase = model.compute_stft(y_target_wav.squeeze())
                    # loss = torch.nn.functional.mse_loss(masked_stft, target_stft)             # MSE Spectro Loss
                    # loss = torch.nn.functional.l1_loss(masked_stft, target_stft)                # L1 Spectro Loss
                    # loss = torch.nn.functional.mse_loss(torch.abs(masked_stft), target_mag)   # Mag Spectro Loss

                    masked_mag = torch.abs(masked_stft)
                    masked_mag_db = util.mag2db(masked_mag)
                    target_mag_db = util.mag2db(target_mag)
                    mse_loss = nn.functional.mse_loss(masked_mag_db, target_mag_db)
                    normalized_mse_loss = mse_loss / target_mag.numel()
                    loss = normalized_mse_loss

                    stem_losses.append(loss)
                    
                total_loss = (torch.sum(torch.stack(stem_losses)) / stem_number)
                running_loss += total_loss
                total_loss.backward()
            optimizer.step()

        last_loss = running_loss / (len(train_loader) * batch_size) # average loss for one epoch
        print("loss: {}".format(last_loss))
        writer.add_scalar("Loss/train", last_loss, epoch)

        ## Validation ##
        model.train(False)
        running_vloss = 0.0
        last_vloss = 0
        for val_ind, vdata in enumerate(validation_loader):
            vinputs_batch, vtarget_batch = vdata

            for i in range(batch_size_val):
                vinputs = vinputs_batch[i].to(device)
                voutputs = model(vinputs)
                stem_val = []

                for name, masked_stft in voutputs.items():
                    y_target_wav = vtarget_batch[name][i].to(device)

                    ### STFT ###
                    target_stft, target_mag, target_phase = model.compute_stft(y_target_wav.squeeze())
                    # vloss = torch.nn.functional.mse_loss(masked_stft, target_stft)
                    # vloss = torch.nn.functional.l1_loss(masked_stft, target_stft)
                    # vloss = torch.nn.functional.mse_loss(torch.abs(masked_stft), target_mag)

                    masked_mag = torch.abs(masked_stft)
                    masked_mag_db = util.mag2db(masked_mag)
                    target_mag_db = util.mag2db(target_mag)
                    mse_loss = nn.functional.mse_loss(masked_mag_db, target_mag_db)
                    normalized_mse_loss = mse_loss / target_mag.numel()
                    vloss = normalized_mse_loss

                    stem_val.append(vloss)

                total_vloss = (torch.sum(torch.stack(stem_val)) / stem_number)
                running_vloss += total_vloss
        
        last_vloss = running_vloss / (len(validation_loader) * batch_size_val)
        print("val loss: {}".format(last_vloss))
        # scheduler.step(last_vloss)
        scheduler.step()
        val_writer.add_scalar("Val/train", last_vloss, epoch)
        print("------------------------------------------------------------------------------------")
    print("Finished training on: {}".format(datetime.now().strftime("%Y%m%d-%H")))

    # torch.save({'epoch': epochs,
    #         'model_state_dict': model.state_dict(),
    #         'optimizer_state_dict': optimizer.state_dict(),
    #         'loss': last_loss}, 
    #         "/home/ytang363/7100_spr2023/model/{}_ep-{}_b-{}.pt".format(now_str, epochs,batch_size))
    
    torch.save({'epoch': epochs,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': last_loss,
            'scheduler': scheduler.state_dict()}, 
            "/home/ytang363/7100_spr2023/model/{}_ep-{}_b-{}.pt".format(now_str, epochs,batch_size))
    
    print("saved model")

def trainwSDR(
    dataset: str = "/home/ytang363/7100_spr2023/musdb18",
    output_dir: str = "/home/ytang363/7100_spr2023/model",
    epochs: int = 100,
    batch_size: int = 16,
    batch_size_val: int = 8,
    learning_rate: float = 1e-3,
    drop: bool = True,
    pretrain: bool = False,
    model_dir: str = "/home/ytang363/7100_spr2023/model/20230313-22_ep-500_b-4.pt",
    stem_number: int = 2
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("total epochs: {}; batch size: {}".format(epochs, batch_size))

    train_dataset = musdbDataset(root=dataset, is_train=True, split="train")
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=drop)
    validation_dataset = musdbDataset(root=dataset, is_train=True, split="valid")
    validation_loader = DataLoader(validation_dataset, batch_size=batch_size_val, shuffle=True, drop_last=drop)

    model = Splitter(stem_names=[s for s in train_dataset.targets]).to(device)
    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=1e-5)
    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=100,threshold=1e-1,min_lr=1e-7)
    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.1)

    # Continue Training
    if pretrain:
        print("Continue Training")
        checkpoint = torch.load(model_dir)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        loss = checkpoint['loss']

    now_str = datetime.now().strftime("%Y%m%d-%H")
    writer = SummaryWriter("/home/ytang363/7100_spr2023/logs-sdr/logs_loss_sdr/{}_e-100".format(now_str))
    # val_writer = SummaryWriter("/home/ytang363/7100_spr2023/logs-sdr/logs_loss_sdr/{}_val".format(now_str))
    SDR_writer = SummaryWriter("/home/ytang363/7100_spr2023/logs-sdr/logs_loss_sdr/{}_sdr_e-100".format(now_str))
    
    for epoch in range(epochs):
        print("Epoch {}:".format(epoch+1))

        ## Training ##
        model.train(True)
        running_loss = 0
        last_loss = 0
        batch_iterator = tqdm(train_loader, desc="Batch")
        for batch_idx, batch in enumerate(batch_iterator):
            x_wav_batch, y_target_wavs_batch = batch
            optimizer.zero_grad()

            for i in range(batch_size):
                x_wav = x_wav_batch[i].to(device)
                predictions = model(x_wav)
                stem_losses = []

                for name, masked_stft in predictions.items():
                    y_target_wav = y_target_wavs_batch[name][i].to(device)
                    target_stft, _ = model.compute_stft(y_target_wav.squeeze())
                    loss = torch.nn.functional.mse_loss(masked_stft, target_stft)
                    stem_losses.append(loss)
                    # running_loss += loss.item()
                    
                total_loss = (torch.sum(torch.stack(stem_losses)) / stem_number)
                running_loss += total_loss
                total_loss.backward()
            optimizer.step()

        last_loss = running_loss / (len(train_loader) * batch_size) # average loss for one epoch
        print("loss: {}".format(last_loss))
        writer.add_scalar("Loss/train", last_loss, epoch)

        # ## Validation ##
        # model.train(False)
        # running_vloss = 0.0
        # last_vloss = 0
        # for val_ind, vdata in enumerate(validation_loader):
        #     vinputs_batch, vtarget_batch = vdata

        #     for i in range(batch_size_val):
        #         vinputs = vinputs_batch[i].to(device)
        #         voutputs = model(vinputs)
        #         stem_val = []

        #         for name, masked_stft in voutputs.items():
        #             y_target_wav = vtarget_batch[name][i].to(device)
        #             target_stft, _ = model.compute_stft(y_target_wav.squeeze())
        #             vloss = torch.nn.functional.mse_loss(masked_stft, target_stft)
        #             stem_val.append(vloss)

        #         total_vloss = (torch.sum(torch.stack(stem_val)) / stem_number)
        #         running_vloss += total_vloss
        
        # last_vloss = running_vloss / (len(validation_loader) * batch_size_val)
        # print("val loss: {}".format(last_vloss))
        # # scheduler.step(last_vloss)
        # # scheduler.step()
        # val_writer.add_scalar("Val/train", last_vloss, epoch)

        ## Calculate SDR ##
        util = utility()
        sdr = util.cal_SDR(model=model, duration=60, niter=3, shuffle=True)
        sdr_mean = np.mean(sdr)
        print("SDR: {}".format(sdr_mean))
        SDR_writer.add_scalar("SDR", sdr_mean, epoch)
        print("------------------------------------------------------------------------------------")

    print("Finished training on: {}".format(datetime.now().strftime("%Y%m%d-%H")))

    torch.save({'epoch': epochs,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': last_loss}, 
            "/home/ytang363/7100_spr2023/model/evl-{}_ep-{}_b-{}.pt".format(now_str, epochs,batch_size))
    
    print("saved model")

if __name__ == "__main__":
    if torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"
    print(f"Using {device}")

    train()
    # trainwSDR()
        